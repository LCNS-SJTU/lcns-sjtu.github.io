---
---

@string{aps = {American Physical Society,}}

@article{Song2021,
  abbr = {Cereb. Cortex},
  bibtex_show={true},
  abstract = {A brain network comprises a substantial amount of short-range connections with an admixture of long-range connections. The portion of long-range connections in brain networks is observed to be quantitatively dissimilar across species. It is hypothesized that the length of connections is constrained by the spatial embedding of brain networks, yet fundamental principles that underlie the wiring length distribution remain unclear. By quantifying the structural diversity of a brain network using Shannon's entropy, here we show that the wiring length distribution across multiple species—including Drosophila, mouse, macaque, human, and C. elegans—follows the maximum entropy principle (MAP) under the constraints of limited wiring material and the spatial locations of brain areas or neurons. In addition, by considering stochastic axonal growth, we propose a network formation process capable of reproducing wiring length distributions of the 5 species, thereby implementing MAP in a biologically plausible manner. We further develop a generative model incorporating MAP, and show that, for the 5 species, the generated network exhibits high similarity to the real network. Our work indicates that the brain connectivity evolves to be structurally diversified by maximizing entropy to support efficient interareal communication, providing a potential organizational principle of brain networks.},
  author = {Song, Yuru and Zhou, Douglas and Li, Songting},
  doi = {10.1093/cercor/bhab110},
  issn = {1047-3211},
  journal = {Cereb. Cortex},
  month = {aug},
  number = {10},
  pages = {4628--4641},
  title = {{Maximum Entropy Principle Underlies Wiring Length Distribution in Brain Networks}},
  html = {https://academic.oup.com/cercor/article/31/10/4628/6276649},
  volume = {31},
  year = {2021},
  pdf = {Song_Zhou_Li_CerebralCortex_2021.pdf},
}

@article{Li2019,
  abbr = {PNAS},
  bibtex_show={true},
  abstract = {Complex dendrites in general present formidable challenges to understanding neuronal information processing. To circumvent the difficulty, a prevalent viewpoint simplifies the neuronal morphology as a point representing the soma, and the excitatory and inhibitory synaptic currents originated from the dendrites are treated as linearly summed at the soma. Despite its extensive applications, the validity of the synaptic current description remains unclear, and the existing point neuron framework fails to characterize the spatiotemporal aspects of dendritic integration supporting specific computations. Using electrophysiological experiments, realistic neuronal simulations, and theoretical analyses, we demonstrate that the traditional assumption of linear summation of synaptic currents is oversimplified and underestimates the inhibition effect. We then derive a form of synaptic integration current within the point neuron framework to capture dendritic effects. In the derived form, the interaction between each pair of synaptic inputs on the dendrites can be reliably parameterized by a single coefficient, suggesting the inherent low-dimensional structure of dendritic integration. We further generalize the form of synaptic integration current to capture the spatiotemporal interactions among multiple synaptic inputs and show that a point neuron model with the synaptic integration current incorporated possesses the computational ability of a spatial neuron with dendrites, including direction selectivity, coincidence detection, logical operation, and a bilinear dendritic integration rule discovered in experiment. Our work amends the modeling of synaptic inputs and improves the computational power of a modeling neuron within the point neuron framework.},
  author = {Li, Songting and Liu, Nan and Zhang, Xiaohui and McLaughlin, David W. and Zhou, Douglas and Cai, David},
  doi = {10.1073/pnas.1904463116},
  issn = {0027-8424},
  journal = {Proc. Natl. Acad. Sci.},
  month = {jul},
  number = {30},
  pages = {15244--15252},
  title = {{Dendritic computations captured by an effective point neuron model}},
  html = {http://www.pnas.org/lookup/doi/10.1073/pnas.1904463116},
  volume = {116},
  year = {2019}
}

@article{Li2021,
  abbr = {CPAM},
  bibtex_show={true},
  author = {Li, Songting and McLaughlin, David W. and Zhou, Douglas},
  doi = {10.1002/cpa.22020},
  issn = {0010-3640},
  journal = {Commun. Pure Appl. Math.},
  month = {aug},
  pages = {cpa.22020},
  title = {{Mathematical Modeling and Analysis of Spatial Neuron Dynamics: Dendritic Integration and Beyond}},
  html = {https://onlinelibrary.wiley.com/doi/10.1002/cpa.22020},
  year = {2021},
}

@article{li2018causal,
abstract = {The Granger causality (GC) analysis has been extensively applied to infer causal interactions in dynamical systems arising from economy and finance, physics, bioinformatics, neuroscience, social science, and many other fields. In the presence of potential nonlinearity in these systems, the validity of the GC analysis in general is questionable. To illustrate this, here we first construct minimal nonlinear systems and show that the GC analysis fails to infer causal relations in these systems - it gives rise to all types of incorrect causal directions. In contrast, we show that the time-delayed mutual information (TDMI) analysis is able to successfully identify the direction of interactions underlying these nonlinear systems. We then apply both methods to neuroscience data collected from experiments and demonstrate that the TDMI analysis but not the GC analysis can identify the direction of interactions among neuronal signals. Our work exemplifies inference hazards in the GC analysis in nonlinear systems and suggests that the TDMI analysis can be an appropriate tool in such a case.},
author = {Li, Songting and Xiao, Yanyang and Zhou, Douglas and Cai, David},
doi = {10.1103/PhysRevE.97.052216},
issn = {2470-0045},
journal = {Phys. Rev. E},
month = {may},
number = {5},
pages = {052216},
pmid = {29906860},
publisher = {APS},
title = {{Causal inference in nonlinear systems: Granger causality versus time-delayed mutual information}},
html = {https://link.aps.org/doi/10.1103/PhysRevE.97.052216},
volume = {97},
year = {2018}
}
@article{Li2017,
abstract = {Interneurons are important for computation in the brain, in particular, in the information processing involving the generation of theta oscillations in the hippocampus. Yet the functional role of interneurons in the theta generation remains to be elucidated. Here we use time-delayed mutual information to investigate information flow related to a special class of interneurons-theta-driving neurons in the hippocampal CA1 region of the mouse-to characterize the interactions between theta-driving neurons and theta oscillations. For freely behaving mice, our results show that information flows from the activity of theta-driving neurons to the theta wave, and the firing activity of theta-driving neurons shares a substantial amount of information with the theta wave regardless of behavioral states. Via realistic simulations of a CA1 pyramidal neuron, we further demonstrate that theta-driving neurons possess the characteristics of the cholecystokinin-expressing basket cells (CCK-BC). Our results suggest that it is important to take into account the role of CCK-BC in the generation and information processing of theta oscillations.},
author = {Li, Songting and Xu, Jiamin and Chen, Guifen and Lin, Longnian and Zhou, Douglas and Cai, David},
doi = {10.1038/s41598-017-05527-2},
issn = {20452322},
journal = {Sci. Rep.},
number = {1},
pages = {1--12},
publisher = {Springer US},
title = {{The characterization of hippocampal theta-driving neurons-A time-delayed mutual information approach}},
html = {http://dx.doi.org/10.1038/s41598-017-05527-2},
volume = {7},
year = {2017}
}
@article{Zhou2010,
abstract = {We discuss how to characterize long-time dynamics of non-smooth dynamical systems, such as integrate-and-fire (I{\&}F) like neuronal network, using Lyapunov exponents and present a stable numerical method for the accurate evaluation of the spectrum of Lyapunov exponents for this large class of dynamics. These dynamics contain (i) jump conditions as in the firing-reset dynamics and (ii) degeneracy such as in the refractory period in which voltage-like variables of the network collapse to a single constant value. Using the networks of linear I{\&}F neurons, exponential I{\&}F neurons, and I{\&}F neurons with adaptive threshold, we illustrate our method and discuss the rich dynamics of these networks. {\textcopyright} Springer Science + Business Media, LLC 2009.},
author = {Zhou, Douglas and Sun, Yi and Rangan, Aaditya V. and Cai, David},
doi = {10.1007/s10827-009-0201-3},
issn = {0929-5313},
journal = {J. Comput. Neurosci.},
keywords = {Firing-reset,Integrate-and-fire,Lyapunov exponents,Non-smooth,Refractory-induced degeneracy},
month = {apr},
number = {2},
pages = {229--245},
pmid = {20012178},
title = {{Spectrum of Lyapunov exponents of non-smooth dynamical systems of integrate-and-fire type}},
html = {http://link.springer.com/10.1007/s10827-009-0201-3},
volume = {28},
year = {2010}
}
@article{Barranca2015,
abstract = {Densely-connected networks are prominent among natural systems, exhibiting structural characteristics often optimized for biological function. To reveal such features in highly-connected networks, we introduce a new network characterization determined by a decomposition of network-connectivity into low-rank and sparse components. Based on these components, we discover a new class of networks we define as amalgamated networks, which exhibit large functional groups and dense connectivity. Analyzing recent experimental findings on cerebral cortex, food-web, and gene regulatory networks, we establish the unique importance of amalgamated networks in fostering biologically advantageous properties, including rapid communication among nodes, structural stability under attacks, and separation of network activity into distinct functional modules. We further observe that our network characterization is scalable with network size and connectivity, thereby identifying robust features significant to diverse physical systems, which are typically undetectable by conventional characterizations of connectivity. We expect that studying the amalgamation properties of biological networks may offer new insights into understanding their structure-function relationships.},
author = {Barranca, Victor J. and Zhou, Douglas and Cai, David},
doi = {10.1038/srep10611},
issn = {2045-2322},
journal = {Sci. Rep.},
month = {sep},
number = {1},
pages = {10611},
pmid = {26035066},
publisher = {Nature Publishing Group},
title = {{A Novel Characterization of Amalgamated Networks in Natural Systems}},
html = {http://dx.doi.org/10.1038/srep10611},
volume = {5},
year = {2015}
}
@article{Barranca2015a,
abstract = {Small-world networks occur naturally throughout biological, technological, and social systems. With their prevalence, it is particularly important to prudently identify small-world networks and further characterize their unique connection structure with respect to network function. In this work we develop a formalism for classifying networks and identifying small-world structure using a decomposition of network connectivity matrices into low-rank and sparse components, corresponding to connections within clusters of highly connected nodes and sparse interconnections between clusters, respectively. We show that the network decomposition is independent of node indexing and define associated bounded measures of connectivity structure, which provide insight into the clustering and regularity of network connections. While many existing network characterizations rely on constructing benchmark networks for comparison or fail to describe the structural properties of relatively densely connected networks, our classification relies only on the intrinsic network structure and is quite robust with respect to changes in connection density, producing stable results across network realizations. Using this framework, we analyze several real-world networks and reveal new structural properties, which are often indiscernible by previously established characterizations of network connectivity.},
author = {Barranca, Victor J. and Zhou, Douglas and Cai, David},
doi = {10.1103/PhysRevE.92.062822},
issn = {1539-3755},
journal = {Phys. Rev. E},
month = {dec},
number = {6},
pages = {062822},
pmid = {26764759},
title = {{Low-rank network decomposition reveals structural characteristics of small-world networks}},
html = {https://link.aps.org/doi/10.1103/PhysRevE.92.062822},
volume = {92},
year = {2015}
}
@article{tian2020exponential,
abstract = {The exponential time differencing (ETD) method allows using a large time step to efficiently evolve stiff systems such as Hodgkin-Huxley (HH) neural networks. For pulse-coupled HH networks, the synaptic spike times cannot be predetermined and are convoluted with neuron's trajectory itself. This presents a challenging issue for the design of an efficient numerical simulation algorithm. The stiffness in the HH equations are quite different, for example, between the spike and non-spike regions. Here, we design a second-order adaptive exponential time differencing algorithm (AETD2) for the numerical evolution of HH neural networks. Compared with the regular second-order Runge-Kutta method (RK2), our AETD2 method can use time steps one order of magnitude larger and improve computational efficiency more than ten times while excellently capturing accurate traces of membrane potentials of HH neurons. This high accuracy and efficiency can be robustly obtained and do not depend on the dynamical regimes, connectivity structure or the network size.},
author = {Tian, Zhong-qi Kyle and Zhou, Douglas},
doi = {10.3389/fncom.2020.00040},
issn = {1662-5188},
journal = {Front. Comput. Neurosci.},
keywords = {Hodgkin-Huxley,efficiency,exponential time differencing method,pulse-coupled,second-order},
month = {may},
pages = {40},
publisher = {Frontiers},
title = {{Exponential Time Differencing Algorithm for Pulse-Coupled Hodgkin-Huxley Neural Networks}},
html = {https://www.frontiersin.org/article/10.3389/fncom.2020.00040/full},
volume = {14},
year = {2020}
}
@article{tian2019digital,
abstract = {Transfer entropy (TE) is an attractive model-free method to detect causality and infer structural connectivity of general digital systems. However it relies on high dimensions used in its definition to clearly remove the memory effect and distinguish the direct causality from the indirect ones which makes it almost inoperable in practice. In this work, we try to use a low order and pairwise TE framework with binary data suitably filtered from the recorded signals to avoid the high dimensional problem. Under this setting, we find and explain that the TE values from the connected and unconnected pairs have a significant difference of magnitude, which can be easily classified by cluster methods. This phenomenon widely and robustly holds over a wide range of systems and dynamical regimes. In addition, we find the TE value is quadratically related to the coupling strength and thus we can establish a quantitative mapping between the causal and structural connectivity.},
archivePrefix = {arXiv},
arxivId = {1905.03972},
author = {Tian, Zhong-Qi Kyle and Zhou, Douglas and Cai, David},
eprint = {1905.03972},
journal = {arXiv Prepr. arXiv1905.03972},
month = {may},
title = {{Digital System Reconstruction by Pairwise Transfer Entropy}},
html = {http://arxiv.org/abs/1905.03972},
year = {2019}
}
@article{sun2010pseudo,
abstract = {We present a numerical analysis of the dynamics of all-to-all coupled Hodgkin-Huxley (HH) neuronal networks with Poisson spike inputs. It is important to point out that, since the dynamical vector of the system contains discontinuous variables, we propose a so-called pseudo-Lyapunov exponent adapted from the classical definition using only continuous dynamical variables, and apply it in our numerical investigation. The numerical results of the largest Lyapunov exponent using this new definition are consistent with the dynamical regimes of the network. Three typical dynamical regimes - asynchronous, chaotic and synchronous, are found as the synaptic coupling strength increases from weak to strong. We use the pseudo-Lyapunov exponent and the power spectrum analysis of voltage traces to characterize the types of the network behavior. In the nonchaotic (asynchronous or synchronous) dynamical regimes, i.e., the weak or strong coupling limits, the pseudo-Lyapunov exponent is negative and there is a good numerical convergence of the solution in the trajectory-wise sense by using our numerical methods. Consequently, in these regimes the evolution of neuronal networks is reliable. For the chaotic dynamical regime with an intermediate strong coupling, the pseudo-Lyapunov exponent is positive, and there is no numerical convergence of the solution and only statistical quantifications of the numerical results are reliable. Finally, we present numerical evidence that the value of pseudo-Lyapunov exponent coincides with that of the standard Lyapunov exponent for systems we have been able to examine. {\textcopyright} Springer Science+Business Media, LLC 2009.},
author = {Sun, Yi and Zhou, Douglas and Rangan, Aaditya V. and Cai, David},
doi = {10.1007/s10827-009-0202-2},
issn = {0929-5313},
journal = {J. Comput. Neurosci.},
keywords = {Chaos,Hodgkin-Huxley neuron,Lyapunov exponents,Neuronal network,Numerical analysis},
month = {apr},
number = {2},
pages = {247--266},
pmid = {20020192},
publisher = {Springer},
title = {{Pseudo-Lyapunov exponents and predictability of Hodgkin-Huxley neuronal network dynamics}},
html = {http://link.springer.com/10.1007/s10827-009-0202-2},
volume = {28},
year = {2010}
}
@article{Zhou2013,
abstract = {We study the reconstruction of structural connectivity for a general class of pulse-coupled nonlinear networks and show that the reconstruction can be successfully achieved through linear Granger causality (GC) analysis. Using spike-triggered correlation of whitened signals, we obtain a quadratic relationship between GC and the network couplings, thus establishing a direct link between the causal connectivity and the structural connectivity within these networks. Our work may provide insight into the applicability of GC in the study of the function of general nonlinear networks. {\textcopyright} 2013 American Physical Society.},
author = {Zhou, Douglas and Xiao, Yanyang and Zhang, Yaoyu and Xu, Zhiqin and Cai, David},
doi = {10.1103/PhysRevLett.111.054102},
issn = {0031-9007},
journal = {Phys. Rev. Lett.},
month = {jul},
number = {5},
pages = {054102},
publisher = {APS},
title = {{Causal and Structural Connectivity of Pulse-Coupled Nonlinear Networks}},
html = {https://link.aps.org/doi/10.1103/PhysRevLett.111.054102},
volume = {111},
year = {2013}
}
@article{Tian2021,
abstract = {The causal connectivity of a network is often inferred to understand the network function. It is arguably acknowledged that the inferred causal connectivity relies on causality measure one applies, and it may differ from the network's underlying structural connectivity. However, the interpretation of causal connectivity remains to be fully clarified, in particular, how causal connectivity depends on causality measures and how causal connectivity relates to structural connectivity. Here, we focus on nonlinear networks with pulse signals as measured output, {\$}e.g.{\$}, neural networks with spike output, and address the above issues based on four intensively utilized causality measures, {\$}i.e.{\$}, time-delayed correlation, time-delayed mutual information, Granger causality, and transfer entropy. We theoretically show how these causality measures are related to one another when applied to pulse signals. Taking the simulated Hodgkin-Huxley neural network and the real mouse brain network as two illustrative examples, we further verify the quantitative relations among the four causality measures and demonstrate that the causal connectivity inferred by any of the four well coincides with the underlying network structural connectivity, therefore establishing a direct link between the causal and structural connectivity. We stress that the structural connectivity of networks can be reconstructed pairwisely without conditioning on the global information of all other nodes in a network, thus circumventing the curse of dimensionality. Our framework provides a practical and effective approach for pulse-output network reconstruction.},
archivePrefix = {arXiv},
arxivId = {2110.09521},
author = {Tian, Zhong-qi K. and Chen, Kai and Li, Songting and McLaughlin, David W. and Zhou, Douglas},
eprint = {2110.09521},
journal = {Nat. Commun.},
month = {oct},
number = {1},
pages = {4950},
title = {{Quantitative relations among causality measures with applications to nonlinear pulse-output network reconstruction}},
html = {http://arxiv.org/abs/2110.09521},
volume = {9},
year = {2021}
}
@article{Dai2018,
abstract = {Recently, sophisticated optogenetic tools for mouse have enabled many detailed studies of the neuronal circuits of its primary visual cortex (V1), providing much more specific information than is available for cat or monkey. Among various other differences, they show a striking contrast dependency in orientation selectivity in mouse V1 rather than the well-known contrast invariance for cat and monkey. Constrained by the existing experiment data, we develop a comprehensive large-scale model of an effective input layer of mouse V1 that successfully reproduces the contrast-dependent phenomena and many other response properties. The model helps to probe different mechanisms based on excitation–inhibition balance that underlie both contrast dependencies and invariance, and it provides implications for future studies on these circuits.},
author = {Dai, Wei P. and Zhou, Douglas and McLaughlin, David W. and Cai, David},
doi = {10.1073/pnas.1719044115},
issn = {0027-8424},
journal = {Proc. Natl. Acad. Sci.},
keywords = {Contrast dependence,Contrast invariance,Orientation selectivity},
month = {nov},
number = {45},
pages = {11619--11624},
pmid = {30337480},
title = {{Mechanisms underlying contrast-dependent orientation selectivity in mouse V1}},
html = {https://pnas.org/doi/full/10.1073/pnas.1719044115},
volume = {115},
year = {2018}
}
@article{Li2014,
abstract = {Neurons process information via integration of synaptic inputs from dendrites. Many experimental results demonstrate dendritic integration could be highly nonlinear, yet few theoretical analyses have been performed to obtain a precise quantitative characterization analytically. Based on asymptotic analysis of a two-compartment passive cable model, given a pair of time-dependent synaptic conductance inputs, we derive a bilinear spatiotemporal dendritic integration rule. The summed somatic potential can be well approximated by the linear summation of the two postsynaptic potentials elicited separately, plus a third additional bilinear term proportional to their product with a proportionality coefficient (Formula presented.) . The rule is valid for a pair of synaptic inputs of all types, including excitation-inhibition, excitation-excitation, and inhibition-inhibition. In addition, the rule is valid during the whole dendritic integration process for a pair of synaptic inputs with arbitrary input time differences and input locations. The coefficient (Formula presented.) is demonstrated to be nearly independent of the input strengths but is dependent on input times and input locations. This rule is then verified through simulation of a realistic pyramidal neuron model and in electrophysiological experiments of rat hippocampal CA1 neurons. The rule is further generalized to describe the spatiotemporal dendritic integration of multiple excitatory and inhibitory synaptic inputs. The integration of multiple inputs can be decomposed into the sum of all possible pairwise integration, where each paired integration obeys the bilinear rule. This decomposition leads to a graph representation of dendritic integration, which can be viewed as functionally sparse.},
author = {Li, Songting and Liu, Nan and hui Zhang, Xiao and Zhou, Douglas and Cai, David},
doi = {10.1371/journal.pcbi.1004014},
issn = {15537358},
journal = {PLoS Comput. Biol.},
number = {12},
title = {{Bilinearity in Spatiotemporal Integration of Synaptic Inputs}},
volume = {10},
year = {2014}
}
@article{Zhou2013,
abstract = {It has been discovered recently in experiments that the dendritic integration of excitatory glutamatergic inputs and inhibitory GABAergic inputs in hippocampus CA1 pyramidal neurons obeys a simple arithmetic rule as VSExp ≈ VEExp + VIExp + kVEExp VIExp, where VSExp, VEExp and VIExp are the respective voltage values of the summed somatic potential, the excitatory postsynaptic potential (EPSP) and the inhibitory postsynaptic potential measured at the time when the EPSP reaches its peak value. Moreover, the shunting coefficient k in this rule only depends on the spatial location but not the amplitude of the excitatory or inhibitory input on the dendrite. In this work, we address the theoretical issue of how much the above dendritic integration rule can be accounted for using subthreshold membrane potential dynamics in the soma as characterized by the conductance-based integrate-and-fire (I{\&}F) model. Then, we propose a simple I{\&}F neuron model that incorporates the spatial dependence of the shunting coefficient k by a phenomenological parametrization. Our analytical and numerical results show that this dendritic-integration-rule-based I{\&}F (DIF) model is able to capture many experimental observations and it also yields predictions that can be used to verify the validity of the DIF model experimentally. In addition, the DIF model incorporates the dendritic integration effects dynamically and is applicable to more general situations than those in experiments in which excitatory and inhibitory inputs occur simultaneously in time. Finally, we generalize the DIF neuronal model to incorporate multiple inputs and obtain a similar dendritic integration rule that is consistent with the results obtained by using a realistic neuronal model with multiple compartments. This generalized DIF model can potentially be used to study network dynamics that may involve effects arising from dendritic integrations. {\textcopyright} 2013 Zhou et al.},
author = {Zhou, Douglas and Li, Songting and Zhang, Xiao-hui and Cai, David},
doi = {10.1371/journal.pone.0053508},
editor = {Chacron, Maurice J.},
issn = {1932-6203},
journal = {PLoS One},
month = {jan},
number = {1},
pages = {e53508},
title = {{Phenomenological Incorporation of Nonlinear Dendritic Integration Using Integrate-and-Fire Neuronal Frameworks}},
html = {https://dx.plos.org/10.1371/journal.pone.0053508},
volume = {8},
year = {2013}
}
@article{Dai2019,
abstract = {We aim to develop fast algorithms for neuronal simulations to capture the dynamics of a neuron with realistic dendritic morphology. To achieve this, we perform the asymptotic analysis on a cable neuron model with branched dendrites. Using the second-order asymptotic solutions, we derive a bilinear dendritic integration rule to characterize the voltage response at the soma when receiving multiple spatiotemporal synaptic inputs from dendrites, with a dependency on the voltage state of the neuron at input arrival times. Based on the derived bilinear rule, we finally propose two fast algorithms and demonstrate numerically that, in comparison with solving the original cable neuron model numerically, the algorithms can reduce the computational cost of simulation for neuronal dynamics enormously while retaining relatively high accuracy in terms of both sub-threshold dynamics and firing statistics.},
author = {Dai, Wei P. and Li, Songting and Zhou, Douglas},
doi = {10.4310/CMS.2019.v17.n5.a7},
issn = {15396746},
journal = {Commun. Math. Sci.},
keywords = {Asymptotic analysis,Bilinear rule,Cable equation,Dendrites,Dendritic integration},
number = {5},
pages = {1313--1331},
title = {{Fast algorithms for simulation of neuronal dynamics based on the bilinear dendritic integration rule}},
html = {https://www.intlpress.com/site/pub/pages/journals/items/cms/content/vols/0017/0005/a007/},
volume = {17},
year = {2019}
}
@article{Gu2019,
abstract = {It is hypothesized that cortical neuronal circuits operate in a global balanced state, i.e., the majority of neurons fire irregularly by receiving balanced inputs of excitation and inhibition. Meanwhile, it has been observed in experiments that sensory information is often sparsely encoded by only a small set of firing neurons, while neurons in the rest of the network are silent. The phenomenon of sparse coding challenges the hypothesis of a global balanced state in the brain. To reconcile this, here we address the issue of whether a balanced state can exist in a small number of firing neurons by taking account of the heterogeneity of network structure such as scale-free and small-world networks. We propose necessary conditions and show that, under these conditions, for sparsely but strongly connected heterogeneous networks with various types of single-neuron dynamics, despite the fact that the whole network receives external inputs, there is a small active subnetwork (active core) inherently embedded within it. The neurons in this active core have relatively high firing rates while the neurons in the rest of the network are quiescent. Surprisingly, although the whole network is heterogeneous and unbalanced, the active core possesses a balanced state and its connectivity structure is close to a homogeneous Erd{\"{o}}s-R{\'{e}}nyi network. The dynamics of the active core can be well-predicted using the Fokker-Planck equation. Our results suggest that the balanced state may be maintained by a small group of spiking neurons embedded in a large heterogeneous network in the brain. The existence of the small active core reconciles the balanced state and the sparse coding, and also provides a potential dynamical scenario underlying sparse coding in neuronal networks.},
author = {Gu, Qing-long L. and Li, Songting and Dai, Wei P. and Zhou, Douglas and Cai, David},
doi = {10.3389/fncom.2018.00109},
issn = {1662-5188},
journal = {Front. Comput. Neurosci.},
keywords = {Active core,Balanced state,Fokker-Planck equation,Heterogeneous,Homogeneous,Sparse coding},
month = {jan},
number = {January},
title = {{Balanced Active Core in Heterogeneous Neuronal Networks}},
html = {https://www.frontiersin.org/article/10.3389/fncom.2018.00109/full},
volume = {12},
year = {2019}
}
@article{Gu2018,
abstract = {Some previous studies have shown that chaotic dynamics in the balanced state, i.e., one with balanced excitatory and inhibitory inputs into cortical neurons, is the underlying mechanism for the irregularity of neural activity. In this work, we focus on networks of current-based integrate-and-fire neurons with delta-pulse coupling. While we show that the balanced state robustly persists in this system within a broad range of parameters, we mathematically prove that the largest Lyapunov exponent of this type of neuronal networks is negative. Therefore, the irregular firing activity can exist in the systemwithout the chaotic dynamics. That is the irregularity of balanced neuronal networks need not arise from chaos.},
author = {Gu, Qing-long L. and Tian, Zhong-qi K. and Kova{\v{c}}i{\v{c}}, Gregor and Zhou, Douglas and Cai, David},
doi = {10.3389/fncom.2018.00047},
issn = {1662-5188},
journal = {Front. Comput. Neurosci.},
keywords = {Balanced state,Chaotic dynamics,Delta-pulse coupling,Irregular activity,Largest Lyapunov exponent},
month = {jun},
number = {June},
pages = {1--9},
title = {{The Dynamics of Balanced Spiking Neuronal Networks Under Poisson Drive Is Not Chaotic}},
html = {https://www.frontiersin.org/article/10.3389/fncom.2018.00047/full},
volume = {12},
year = {2018}
}
@article{Li2022,
abstract = {In the neocortex, while early sensory areas encode and process external inputs rapidly, higher-association areas are endowed with slow dynamics suitable for accumulating information over time. Such a hierarchy of temporal response windows along the cortical hierarchy naturally emerges in a model of multiareal primate cortex. This finding raises the question of why diverse temporal modes are not mixed in roughly the same way across the whole cortex, despite high connection density and an abundance of feedback loops. We investigate this question by mathematically analyzing the anatomically based network model of macaque cortex and theoretically show that three sufficient conditions of synaptic excitation and inhibition give rise to timescale segregation in a hierarchy, a functionally important characteristic of the cortex.},
author = {Li, Songting and Wang, Xiao-Jing},
doi = {10.1073/pnas.2110274119},
issn = {0027-8424},
journal = {Proc. Natl. Acad. Sci.},
keywords = {Detailed excitation–inhibition balance of long-ran,Eigenvector localization,Interareal heterogeneity,Large-scale cortical network,Timescale hierarchy},
month = {feb},
number = {6},
pmid = {35110401},
title = {{Hierarchical timescales in the neocortex: Mathematical mechanism and biological insights}},
html = {https://pnas.org/doi/full/10.1073/pnas.2110274119},
volume = {119},
year = {2022}
}
